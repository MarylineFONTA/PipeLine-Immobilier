# ğŸ¡ Pipeline de DonnÃ©es : Analyse du MarchÃ© Immobilier

Ce projet a pour but de construire un pipeline de donnÃ©es automatisÃ© pour scraper des annonces immobiliÃ¨res, les nettoyer, et les visualiser via un tableau de bord interactif. L'objectif principal est de fournir une solution qui se met Ã  jour quotidiennement de maniÃ¨re autonome.

-----

## ğŸ—ï¸ Architecture du Projet

Le pipeline se compose de trois Ã©tapes principales :

  * **1. Web Scraping ğŸ•·ï¸**

      * Le **spider Scrapy** (`spider.py`) est utilisÃ© pour extraire les informations clÃ©s (prix, surface, etc.) Ã  partir d'un site d'annonces. Les donnÃ©es brutes sont exportÃ©es dans un fichier JSON.

  * **2. IngÃ©nierie des DonnÃ©es ğŸ§¹**

      * Un script Python (`cleaner.py`) utilise la librairie `pandas` pour nettoyer et structurer les donnÃ©es du fichier JSON. Il gÃ¨re la conversion des types de donnÃ©es et les valeurs manquantes, et calcule des mÃ©triques comme le prix au $m^2$. Le rÃ©sultat est sauvegardÃ© dans un fichier CSV (`cleaned_data.csv`), prÃªt Ã  Ãªtre utilisÃ©.

  * **3. Automatisation CI/CD ğŸ¤–**

      * Un pipeline **GitHub Actions** (`.github/workflows/main.yml`) dÃ©clenche l'exÃ©cution du spider et du script de nettoyage de maniÃ¨re quotidienne. Le workflow commite et met Ã  jour le fichier `cleaned_data.csv` dans le dÃ©pÃ´t GitHub.

  * **4. Tableau de bord interactif ğŸ“Š**

      * Une application web, dÃ©veloppÃ©e avec **Streamlit** (`app.py`), lit les donnÃ©es directement depuis le fichier `cleaned_data.csv` du dÃ©pÃ´t. Elle permet une exploration interactive des donnÃ©es via des filtres et des visualisations.

-----

## ğŸ“‚ Structure des Fichiers

  * `src/`
      * `spider.py`: Le spider Scrapy pour la collecte de donnÃ©es brutes.
      * `cleaner.py`: Le script de nettoyage et de transformation des donnÃ©es.
  * `data/`
      * `raw_data.json`: Fichier JSON contenant les donnÃ©es brutes extraites par Scrapy.
      * `cleaned_data.csv`: Le fichier de donnÃ©es final, nettoyÃ© et structurÃ©, utilisÃ© par l'application Streamlit.
  * `.github/workflows/`
      * `main.yml`: Le script GitHub Actions qui orchestre le pipeline CI/CD.
  * `app.py`: Le code de l'application Streamlit pour la visualisation.

-----

## â–¶ï¸ Comment lancer le projet en local

1.  **Cloner le dÃ©pÃ´t** :

    ```bash
    git clone https://github.com/votre-utilisateur/votre-repo.git
    cd votre-repo
    ```

2.  **Installer les dÃ©pendances** :

    ```bash
    pip install -r requirements.txt
    ```

3.  **ExÃ©cuter le pipeline manuellement** :

    ```bash
    python src/spider.py
    python src/cleaner.py
    ```

4.  **Lancer le tableau de bord Streamlit** :

    ```bash
    streamlit run app.py
    ```

-----
